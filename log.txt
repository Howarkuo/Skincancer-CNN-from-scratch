Classical Convolutional Neural Network from scratch

0. initialize
1. data configuration and augmentation only for training (ImageDataGenerator-tensorflow)
2. build the custom cnn model , 
2.1  The Convolution Layers (Conv2D)
2.1 1st convultion block: 32 feature detector (like vertical edges, horizontal lines, color blob)
2.1 the later layers (64,128) combine these to see complex shapes (like the skin texture patterns or leisions)
2.1  (3,3) the kernel size / the golden standard of the smallest window size in deep learning that can capture the notion of left/right and up/down
2.1 activation - recitified linear unit, f(x) = max (0,x), turn negative values to zero so the model learn non-linearity data boudaries without 'vanishing gradient problem'
2.2 The Pooling Layers (MaxPooling2D)
2.2 (2,2) Pool size -> this takes a 2x2 grid of pixels and keeps only the highest value(the max)
2.2 the pooling layers  cut the the image input height and width in half (224x224 -> 112 x112)
2.2 the pooling layers reduce pixels numb for faster training 
2.2 the pooling layers assures translation invariance - if the skin lesion moves slightly left or rightm the max 
"I don't care if the cancer spot is at pixel 50 or pixel 51. As long as it is generally in this area, I will detect it."
"1. With Pooling (Flexible)
The pooling layer says: "I found a sharp edge somewhere in this 2x2 block."

Result: The next layer just knows "Top-Left area has an edge."

Benefit: If the edge moves 1 pixel, the next layer doesn't notice the difference. It's stable.

2. Without Pooling (Rigid)
The convolution layer says: "I found a sharp edge exactly at Pixel (10, 10)."

Result: The next layer expects that edge at exactly (10, 10).

The Problem: If the skin lesion shifts just 1 pixel to the right to (10, 11), the next layer's filter (which is looking at (10, 10)) might effectively say: "I don't see anything here anymore." '

3. The classifier Head (Dense, Dropout)
3.1 Flatten: Converts the 3D block of features (eg. 28*28*128) into a long 1D list of numbers so the Dense layer can read it
3.2 Dense (512): A fully connected layer with 512 neurons, it looks at all the features extracted by the convolutional layers and tries to make sense of them globally
3.2 dropout(0.5):  turn of 50% neurons to prevent overfitting 
3.2 dense (1, activation ='sigmoid') : final output neuron with binary result (0,1)

Mini-VGGNet.

Input: 224x224 Image.

Block 1: Detects simple edges (32 filters). -> Shrink Image.

Block 2: Detects textures/shapes (64 filters). -> Shrink Image.

Block 3: Detects complex lesion parts (128 filters). -> Shrink Image.

Classifier: Aggregates features (Dense 512) -> Throws away 50% noise (Dropout) -> Makes a probability decision (Sigmoid).

# result 


Performance
fast gpu (rtx 8000) but wait for CPU to resize and augment the next batch of images from the hardrive (temp 83 deg)
#batch size 32
may be improve to 128,256

memory	4.2GB / 49GB	Underutilized. You can fit a much larger batch size.

Result

validation accuracy> 78->89
loss: both down consistently, ending around 0.3, This confirms the model is actually learning features, not just memorizing.

discussion
why the train accuracy spiked at epoch 9 (blue line), maybbe because batch size to small and overreact
why the validation (orange) better than training (blue)? maybe the drop out too high (0.5) making the training too hard
